\subsection{Baseline Model}

Our baseline system answers questions from the SQuAD 2.0 validation dataset without retrieval or fine-tuning, using Llama-3.2-3B-Instruct \citep{dubey2024llama} in a zero-shot configuration.

\textbf{Model Configuration}. Each validation example is formatted with three-components: (1) a task instruction defining expected behavior, (2) the context paragraph from SQuAD 2.0, and (3) the question. The model must either extract an answer span from the context or output ``unanswerable`` when the context provides insufficient information. We employ a low-temperature, top-p-constrained decoding strategy (\texttt{temperature=0.1}, \texttt{top\_p=0.9}, \texttt{do\_sample=True}) to enforce extractive behavior, as the task requires selecting answer spans rather than generating free-form text. We set \texttt{max\_new\_tokens=35} based on the distribution of answer token counts in the training dataset. Generated outputs are parsed by extracting text following the ``Answer:'' delimiter; responses containing keywords such as ``unanswerable'' or ``cannot answer'' are classified as abstentions.

\textbf{Instruction Design.} We evaluate three instruction variants on a stratified sample of 2,000 training examples to identify the optimal prompt formulation. Based on BERTScore F1 performance, we select the instruction variant that was designed to be more "explicit", which emphasizes extractive behavior and provides clear abstention guidelines (see Appendix~\ref{app:baseline_prompt} for full methodology and selected instruction).