\subsection{Fine-Tuned Model}

To enable direct comparison with the baseline, we fine-tune the same Llama-3.2-3B-Instruct model using QLoRA \citep{dettmers2023qlora}, a parameter-efficient method that enables fine-tuning of quantized models while preserving performance. We load the base model in 4-bit precision using NormalFloat4 (NF4) quantization with double quantization enabled, reducing memory footprint while maintaining model quality. Computation is performed in bfloat16 precision.

We apply low-rank adapters to both attention layers (\texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}) and MLP layers (\texttt{gate\_proj}, \texttt{up\_proj}, \texttt{down\_proj}), as the QLoRA paper demonstrates that targeting all linear layers improves adaptation quality compared to attention-only configurations. We use rank $r=16$ with scaling factor $\alpha=32$ (following the common heuristic of $\alpha = 2r$), and dropout of 0.05. This configuration yields 24.3M trainable parameters (0.75\% of the 3.2B total), enabling efficient fine-tuning on a single GPU.

We construct training examples from the SQuAD 2.0 training split by pairing each question-context input with a target: the extracted answer span for answerable questions, or the literal string "unanswerable" for unanswerable questions. This formulation trains the model to both extract answers and recognize knowledge boundaries within a single generation objective. Due to computational constraints, we randomly subsample 8,000 examples from the training split (seed=42) rather than using the full 130K examples. We reserve 5\% of the original training data (6,516 examples) for validation during training.

We train for one epoch to match the baseline's zero-shot setting in terms of minimal task exposure. Training uses a batch size of 1 with gradient accumulation over 8 steps (effective batch size of 8), learning rate of $2 \times 10^{-4}$ with linear decay, and maximum sequence length of 384 tokens. Following the QLoRA paper's recommendation, we mask prompt tokens during loss computation (setting labels to $-100$), training only on the target completion. This focuses learning on answer generation rather than prompt reconstruction.

We evaluate on a randomly sampled subset of 2,000 examples from the SQuAD 2.0 validation split, balanced to include both answerable and unanswerable questions. The same subset is used to evaluate the baseline model, enabling controlled comparison between the two approaches.