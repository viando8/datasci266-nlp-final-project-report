\section{Conclusion}

We presented a systematic comparison of RAG and fine-tuning for unanswerable question detection, evaluating how different knowledge integration paradigms affect abstention behavior in generative LLMs. Using SQuAD 2.0 and a controlled experimental setup with Llama-3.2-3B-Instruct, we isolated the effect of the knowledge integration approach while holding model capacity constant.

\textbf{Key findings}. Both RAG and QLoRA fine-tuning improve unanswerable question detection over the zero-shot baseline, but exhibit distinct accuracy-abstention tradeoffs. QLoRA achieves the highest abstention performance (74.53\% EM on unanswerable questions) but suffers catastrophic degradation on answerable questions (2.99\% EM). RAG offers more balanced performance, improving unanswerable detection to 59.61\% EM while retaining moderate answerable performance (22.22\% EM). These results suggest that externalizing knowledge boundaries through retrieval yields more graceful degradation than encoding them parametrically, where the model learns abstention as a dominant strategy.

\textbf{Limitations}. Our findings are subject to several constraints. The QLoRA model was trained on only 6\% of available training data for a single epoch due to computational limitations, which likely contributed to the over-abstention behavior. The QLoRA evaluation used a smaller validation subset than the baseline and RAG experiments, limiting direct comparability. Additionally, our study uses a single model architecture (Llama 3B) and dataset (SQuAD 2.0); generalization to other models, scales, and domains remains to be validated.

\textbf{Future work}. Several directions emerge from our findings. First, training QLoRA on the full dataset with stratified sampling or weighted loss functions may yield more balanced abstention behavior. Second, hybrid approaches that combine retrieval's external grounding with fine-tuning's learned judgment warrant exploration. Third, multi-task objectives that explicitly balance answer extraction and abstention detection could mitigate the tradeoff we observed. Finally, extending this comparison to domain-specific settings (e.g., medical or legal QA) would assess whether our findings hold in the high-stakes applications that motivated this work.
