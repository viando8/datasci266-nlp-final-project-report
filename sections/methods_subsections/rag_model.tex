\subsection{RAG Model}

To evaluate the efficacy of non-parametric knowledge integration, we implemented a Retrieval-Augmented Generation (RAG) system. Unlike fine-tuning, which encodes knowledge into model weights, RAG grounds generation in explicitly retrieved context. Our architecture consists of two primary components: a dense retriever and a generative reader.

\textbf{Retrieval Component}. We constructed a dense vector index over SQuAD 2.0 context passages to enable semantic search. We utilized \texttt{multi-qa-MiniLM-L6-cos-v1} from the Sentence Transformers library \citep{reimers2019sentencebert}, which maps text into a 384-dimensional dense vector space optimized for semantic search and question-answering tasks. We implemented the index using FAISS (Facebook AI Similarity Search) \citep{johnson2017faiss} with an \texttt{IndexFlatL2} structure, performing exhaustive Euclidean distance searches to guarantee exact nearest-neighbor retrieval without approximation errors. The SQuAD 2.0 dataset was preprocessed to extract unique context paragraphs, yielding approximately 150,000 entries with no duplicates.

\textbf{Generation Pipeline}. The generation process follows a retrieve-then-generate workflow. For each query, the system retrieves the top $k=3$ most relevant context paragraphs. We construct a prompt concatenating these retrieved passages with the same instruction used in the baseline, explicitly stating: ``If the answer is not present in the provided context, you must abstain.'' We use the same Llama-3.2-3B-Instruct base model as the baseline and fine-tuned experiments to ensure fair comparison.

\textbf{Abstention Logic}. To detect unanswerable questions, we implemented a two-stage filter. First, the model is instructed to output a specific abstention token. Second, a fallback mechanism checks if the answer generation fails to follow the extractive format, classifying such failures as ``unanswerable''.