\subsection{Dataset Description}

We evaluate all systems on SQuAD 2.0 \citep{rajpurkar2018squad2}, which includes 50,000 unanswerable questions alongside 100,000+ answerable ones. This dataset requires systems to determine both when they can answer and when they should abstain.

The dataset consists of question-answer pairs derived from Wikipedia articles across 477 distinct topics. Each example includes a context paragraph, a question, and either an extracted answer span or an empty array indicating the question is unanswerable. We use the official training split for model training and fine-tuning, and reserve the official validation split as our held-out test set.

\textbf{Data Characteristics}. Table~\ref{tab:squad_composition} shows the dataset composition by answerability. The training set contains 130,319 examples with 66.6\% answerable questions. In contrast, the validation set is evenly balanced with 50.1\% unanswerable questions across 11,873 examplesâ€”a distribution that challenges models to recognize knowledge boundaries without relying on dataset bias.

\begin{table}[t]
\centering
\caption{SQuAD 2.0 dataset composition by answerability.}
\label{tab:squad_composition}
\begin{tabular}{llr}
\toprule
\textbf{Split} & \textbf{Type} & \textbf{Count} \\
\midrule
\multirow{3}{*}{Training} 
    & Answerable & 86,821 (66.6\%) \\
    & Unanswerable & 43,498 (33.4\%) \\
    & \textit{Total} & \textit{130,319} \\
\midrule
\multirow{3}{*}{Validation} 
    & Answerable & 5,928 (49.9\%) \\
    & Unanswerable & 5,945 (50.1\%) \\
    & \textit{Total} & \textit{11,873} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:squad_tokens} presents token count statistics for questions, contexts, and answers. Questions are consistently brief, with a median length of 12 tokens across both splits. Context paragraphs are substantially longer, with median lengths of 146 tokens (training) and 149 tokens (validation). Answer spans for answerable questions are short, with a median of 3 tokens, though the distribution has a long tail extending to 75 tokens in the training set.

\begin{table}[t]
\centering
\caption{SQuAD 2.0 token count statistics using Llama-3.2-3B tokenizer. Mean values are shown with median in parentheses.}
\label{tab:squad_tokens}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Component} & \textbf{Split} & \textbf{Mean (Med.)} & \textbf{Range} \\
\midrule
\multirow{2}{*}{Question} 
    & Train & 12.32 (12) & 1--61 \\
    & Val & 12.37 (12) & 4--37 \\
\midrule
\multirow{2}{*}{Context} 
    & Train & 158.61 (146) & 26--907 \\
    & Val & 166.90 (149) & 30--780 \\
\midrule
\multirow{2}{*}{Answer} 
    & Train & 4.69 (3) & 1--75 \\
    & Val & 4.40 (3) & 1--42 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{QLoRA subsets}. Due to computational constraints, we train the QLoRA model on a randomly sampled subset of 8,000 training examples (6.1\% of full data, \texttt{seed=42}) and evaluate on a 2,000-example subset of the validation split. This reduced scale may limit the model's exposure to the full diversity of question types, a limitation we address in future work.
