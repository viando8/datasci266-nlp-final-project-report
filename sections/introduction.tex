\section{Introduction}

Question answering (QA) represents a primary use of large language models (LLMs), especially in domains that require accurate information. However, QA systems face a fundamental challenge that becomes particularly significant in high-stakes applications: they often hallucinate answers to unanswerable questions instead of abstaining \citep{kalai2025hallucinate}.

Healthcare and legal domains are particularly critical. QA systems for consumer health queries, clinical decision support, and drug interaction checking must never fabricate information, as hallucinated medical advice can have life-threatening consequences \citep{pal2023medhalt}. Similarly, systems for legal research, compliance checking, and contract analysis can cause penalties, liability, or rights violations through fabricated answers. Prominent scandals have occurred where AI systems invented non-existent legal cases, resulting in attorney sanctions \citep{dahl2024legal}. These domains particularly need robust unanswerable question detection because knowledge bases have clear boundaries yet cannot cover every scenario. Medical guidelines cannot address every rare disease combination; legal databases cannot cover every novel situation. When questions fall outside a system's knowledge scope, it must recognize this rather than fabricate answers.

Two prominent paradigms address the hallucination challenge through different approaches to knowledge integration. Retrieval-Augmented Generation (RAG) retrieves relevant passages from a knowledge base before generation, representing an external, non-parametric approach to grounding responses \citep{lewis2020rag, shuster2021raghallucination}. Fine-tuning directly encodes knowledge into model parameters through training on domain-specific question-answer pairs, including unanswerable examples, representing an internal, parametric approach \citep{roberts2020fine}. While other mitigation techniques exist - including prompt engineering, chain-of-thought reasoning \citep{wei2022cot}, and human-in-the-loop verification \citep{ouyang2022human} - RAG and fine-tuning represent the two fundamental paradigms for integrating domain knowledge into QA systems.

Organizations deploying QA systems must choose between RAG and fine-tuning for knowledge integration, yet lack empirical guidance on which approach better handles unanswerable questions. Specifically, we address: \textit{Do models more reliably abstain from answering when knowledge is provided externally through retrieval (RAG) or encoded internally through fine-tuning? How does each paradigm balance answering answerable questions correctly while recognizing when questions fall outside the knowledge scope?} These questions are critical for high-stakes deployments where incorrect answers carry significant consequences.

While prior work has compared RAG and fine-tuning on standard QA metrics like exact match and F1 scores, these comparisons focus primarily on accuracy when answers exist. Little empirical research examines how these paradigms differ in their ability to detect unanswerable questions and appropriately abstain \citep{ovadia2024ftvsrag, balaguer2024ftvsragag}. Furthermore, existing hallucination detection work focuses on identifying fabricated content after generation, rather than comparing prevention strategies through different knowledge integration approaches \citep{sadat2023delucionqa, farquhar2024semantic}. Finally, studies on SQuAD 2.0 - a dataset explicitly designed to test abstention behavior - concentrate on architectural improvements to extractive QA models rather than comparing fundamental knowledge integration paradigms \citep{rajpurkar2018squad}. Our work fills this gap through systematic evaluation of RAG versus fine-tuning specifically on unanswerable question detection.

We present a systematic comparison of RAG and fine-tuning paradigms specifically focused on unanswerable question detection using SQuAD 2.0. Beyond standard metrics, we introduce an answer attribution score that measures whether generated answers are grounded in provided/retrieved context versus hallucinated from parametric memory. This metric directly captures the key distinction between external and internal knowledge integration. We implement three systems using Llama models: (1) zero-shot baseline with no retrieval or fine-tuning, (2) RAG system with dense retrieval from the SQuAD 2.0 corpus, and (3) Llama fine-tuned on SQuAD 2.0 using QLoRA. Our controlled experimental design isolates the effect of knowledge integration paradigm on abstention behavior while maintaining comparable model capacity and training data.

Our evaluation reveals distinct tradeoffs between RAG and fine-tuning for unanswerable question detection...