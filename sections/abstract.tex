\begin{abstract}

Question answering systems must recognize when questions are unanswerable to avoid hallucinating responses, yet empirical guidance on knowledge integration paradigms for abstention remains limited. We present a comparison of Retrieval-Augemented Generation (RAG) versus parameter-efficient fine-tuning (QLoRA) for unanswerable question detection, evaluating how external versus internal knowledge integration affects abstention behavior in generative LLMs. Using SQuAD 2.0 and Llama-3.2-3B-Instruct, our results reveal a critical instability in low-resource fine-tuning: While QLoRA peaked in unanswerable detection (74.53\% EM), this came at the cost of functional performance, as it effectively stopped answering valid questions (2.99\% EM). In contrast, the RAG system provided a robust alternative, improving unanswerable detection to 59.61\% (vs. 52.01\% baseline) while retaining functional answering capabilities (22.22\% EM). Beyond standard metrics, we employ BERTScore and Semantic Textual Similarity to assess answer grounding. Our findings suggest that externalizing knowledge boundaries through retrieval yields more graceful degradation than parametric encoding, where models learn abstention as a dominant strategy, motivating future hybrid approaches.

\end{abstract}