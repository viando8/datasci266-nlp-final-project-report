\section{Background}

Question answering systems have evolved significantly with the introduction of large-scale benchmarks. \citet{rajpurkar2016squad1} introduced the Stanford Question Answering Dataset (SQuAD 1.0), a reading comprehension benchmark containing over 100,000 questions where all answers could be extracted from provided Wikipedia passages. However, SQuAD 1.0 had a critical limitation: every question was guaranteed to be answerable from the given context, meaning systems never needed to recognize when they lacked sufficient information. To address this, \citet{rajpurkar2018squad2} released SQuAD 2.0, augmenting the dataset with over 50,000 unanswerable questions. This established the paradigm of evaluating not just answer accuracy, but also the crucial ability to recognize knowledge boundaries—a capability we leverage to evaluate how different knowledge integration paradigms handle unanswerable questions.

Early approaches to unanswerable question detection in SQuAD 2.0 focused on architectural modifications to extractive models. These methods typically added answer verification components, threshold-based mechanisms, or modified BERT-based architectures to output ``no answer'' predictions when appropriate. However, this line of work concentrated on improving specific model architectures for extractive QA rather than examining fundamental differences in how knowledge is integrated into systems—the core question we address for modern generative LLMs.