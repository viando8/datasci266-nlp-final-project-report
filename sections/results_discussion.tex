\section{Results and Discussion}

Table~\ref{tab:models_comparison} presents the performance comparison between the zero-shot baseline Llama-3.2-3B-Instruct model, the RAG-based system, and the QLoRA fine-tuned variant across all evaluation metrics.\footnote{Baseline and RAG metrics are computed on the full SQuAD 2.0 validation set (11,873 examples), while QLoRA metrics are from a 2,000-example subset. Future work will evaluate all models on identical subsets for stricter comparison.}

\begin{table*}[t]
\centering
\caption{Baseline vs. RAG vs. QLoRA fine-tuned model performance on SQuAD 2.0.}
\label{tab:models_comparison}
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Metric} & \multicolumn{2}{c}{\textbf{Baseline 3B}} & \multicolumn{2}{c}{\textbf{RAG 3B}} & \multicolumn{2}{c}{\textbf{QLoRA 3B}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Ans. & Unans. & Ans. & Unans. & Ans. & Unans. \\
\midrule
Exact Match & 38.36 & 52.01 & 22.22 & 59.61 & 2.99 & 74.53 \\
F1 Score & 59.58 & 52.01 & 39.41 & 59.61 & 18.93 & 74.53 \\
BERTScore F1 & 46.45 & 44.26 & 24.23 & 52.60 & 10.08 & 67.50 \\
STS Score & 85.83 & 78.41 & 76.50 & 81.87 & 71.14 & 88.38 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Comparing Baseline with RAG}

We evaluated both the Zero-Shot Baseline (Llama 3B) and the RAG System on the SQuAD 2.0 validation set. The analysis focuses on three key areas: overall performance, the ability to answer correctly when possible, and the ability to abstain when necessary.

The most significant finding is that the RAG system is significantly superior at handling unanswerable questions (+7.60\% improvement in EM). The RAG system acts "safely": when it cannot find a semantic match in the retrieved documents, it correctly abstains nearly 60% of the time. 

However, the RAG system suffered a significant performance drop on Answerable questions (22.22\% EM vs. Baseline's 38.36\%). This highlights a critical "retrieval bottleneck." Even when the context window was expanded (from k=3 to k=5), the system frequently failed to generate the correct answer. This suggests that when the retriever fails to surface the exact evidence required, the LLM is forced to abstain due to strict prompt instructions, whereas the Baseline can leverage its internal training data to answer correctly.

Additionally, the Baseline achieved a higher BERTScore F1 (45.35\%) compared to RAG (38.57\%). This indicates that when the Baseline generates an answer, it is semantically closer to the ground truth. The lower score for RAG implies that when it does attempt to answer (rather than abstaining), the retrieved context might occasionally mislead the generation.

In summary, the RAG architecture successfully met the project's primary objective of improving unanswerable question detection. By explicitly grounding the generation in retrieved context, we reduced the rate of hallucination on unanswerable queries compared to the baseline. However, this came at the cost of sensitivity; the system became over-cautious, frequently abstaining on questions it could have answered, simply because the retrieval step was imperfect.

\subsection{Comparing Baseline with Q-Lora}

The QLoRA fine-tuned model reveals a fundamental tradeoff in teaching LLMs to recognize knowledge boundaries. Fine-tuning dramatically improved unanswerable question detection---Exact Match rose from 52.01\% to 74.53\%, with BERTScore F1 increasing from 44.26\% to 67.50\%---demonstrating that the model successfully learned to abstain when context is insufficient. However, this capability came at the expense of answer extraction: answerable Exact Match fell from 38.36\% to 2.99\%, and F1 from 59.58\% to 18.93\%. The STS scores reinforce this pattern: high semantic similarity on unanswerable questions (88.38\%) confirms reliable abstention behavior, while lower answerable STS (71.14\%) reflects the model's tendency to abstain even when answers exist. This asymmetry suggests that within our constrained training regime (8,000 randomly sampled examples, single epoch), the model learned abstention as a dominant strategy rather than developing balanced judgment. The finding highlights a key challenge for fine-tuning approaches: optimizing for unanswerable question detection may inadvertently suppress the extractive capabilities that make QA systems useful, a tradeoff that retrieval-augmented approaches may handle differently by externalizing knowledge boundaries.
