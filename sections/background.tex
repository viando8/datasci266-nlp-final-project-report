\section{Background}

\textbf{SQuAD datasets}. QA systems have evolved significantly with large-scale benchmarks. \citet{rajpurkar2016squad1} introduced SQuAD 1.0 with 100,000+ answerable questions from Wikipedia. However, SQuAD 1.0 had a critical limitation: every question was guaranteed to be answerable from the given context, meaning systems never needed to recognize when they lacked sufficient information. To address this, \citet{rajpurkar2018squad2} released SQuAD 2.0, adding 50,000 unanswerable questions. We leverage these unanswerable questions to evaluate how different knowledge integration methods handle abstention behavior.

\textbf{LLM hallucination}. The tendency of LLMs to hallucinate is a fundamental challenge in deployment \citep{ji2023hallucinate}. While post-generation detection methods exist \citep{farquhar2024semantic, sadat2023delucionqa}, our work examines prevention through different knowledge integration paradigms, specifically comparing how RAG versus fine-tuning affect hallucination rates when questions are unanswerable.

\textbf{Retrieval-Augmented Generation}. \citet{lewis2020rag} introduced RAG, which retrieves relevant documents using dense passage retrieval, then conditions generation on query and retrieved passages. This provides non-parametric knowledge access—information stored externally and provided at inference rather than encoded in weights. \citet{shuster2021raghallucination} found RAG reduces fabricated information in dialogue, suggesting external knowledge helps models distinguish accessible versus lacking information.

\textbf{Fine-tuning}. Fine-tuning represents an alternative paradigm where domain knowledge is encoded directly into model parameters through continued training on task-specific data. \citet{roberts2020fine} demonstrated fine-tuning effectively injects factual knowledge into parameters for closed-book QA. This parametric approach internalizes knowledge within the model weights themselves.

\textbf{PEFT}. Parameter-efficient methods like LoRA \citep{hu2022lora} and QLoRA \citep{dettmers2023qlora} made fine-tuning practical for large models by updating  only small matrices while freezing pretrained weights. These methods enable training on domain data including unanswerable examples, potentially teaching boundary recognition. We adopt QLoRA in our fine-tuning experiments to ensure computational feasibility while maintaining model quality.

\textbf{RAG-FT comparisons}. \citet{soudani2024ftvsrag} compared these paradigms for QA over less popular factual knowledge, evaluating twelve language models on Wikipedia-based datasets using substring matching accuracy. They found RAG substantially outperforms fine-tuning across all popularity levels, with the largest gains for least popular entities. However, all questions in their evaluation were answerable from their data—they did not examine abstention behavior when questions are unanswerable.

\textbf{Our positioning}. We address a gap in existing comparisons: how RAG versus fine-tuning handle unanswerable question detection in generative LLMs. While prior comparisons \citep{soudani2024ftvsrag} evaluated QA performance when answers exist in the knowledge base, we examine abstention behavior using SQuAD 2.0's unanswerable questions. Our experimental setup compares three systems—zero-shot baseline, RAG with dense retrieval, and QLoRA fine-tuning—using the same base model and training data to isolate the effect of the knowledge integration paradigm. Beyond standard metrics (EM, F1), we use BERTScore \citep{zhang2020bertscore} and STS \citep{cer2017semeval} to measure whether generated answers are grounded in provided/retrieved context versus hallucinated from parametric memory, directly quantifying the distinction between external and internal knowledge access.
