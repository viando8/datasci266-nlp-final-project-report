\section{Introduction}

Question answering (QA) represents a primary use of large language models (LLMs), especially in domains that require accurate information. However, QA systems face a fundamental challenge: they often hallucinate answers to unanswerable questions instead of abstaining \citep{ji2023hallucinate}.

This challenge becomes critical in high-stakes domains. Healthcare systems for clinical decision support must never fabricate medical advice, as hallucinations can have life-threatening consequences \citep{pal2023medhalt}. Legal research systems that invent non-existence cases have resulted in attorney sanctions \citep{dahl2024legal}. These domains particularly need robust unanswerable question detection because their knowledge bases have clear boundaries - medical guidelines cannot address every rare disease combination; legal databses cannot cover every novel situation.

Two paradigms address hallucination through different knowledge integration approaches. Retrieval-Augmented Generation (RAG) retrieves relevant passages before generation, providing external, non-parametric knowledge access \citep{lewis2020rag}. Fine-tuning encodes knowledge directly into model parameters through training on domain-specific data, representing internal, parametric integration \citep{roberts2020fine}.

Organizations deploying QA systems must choose between these paradigms yet lack empirical guidance on which better handles unanswerable questions. We address: \textit{Do models more reliably abstain when knowledge is provided externally (RAG) or encoded internally (fine-tuning)? How does each paradigm balance answering correctly while recognizing knowledge boundaries?}

\textbf{Gap in existing work}. While prior work compares RAG and fine-tuning on standard QA metrics \citep{ovadia2024ftvsrag}, this focuses on accuracy when answers exist, not on abstention behavior. Hallucination detection work addresses fabrications after generation \citep{sadat2023delucionqa, farquhar2024semantic}, rather than comparing prevention through different knowledge integration approaches. SQuAD 2.0 studies concentrate on architectural improvements to extractive models \citep{rajpurkar2018squad2}, not paradigm comparisons for generative LLMs.

\textbf{Our contribution}. We present a systematic comparison of RAG versus fine-tuning focused on unanswerable question detection. Using SQuAD 2.0, we evaluate three systems: (1) zero-shot baseline with no retrieval or fine-tuning, (2) RAG system with dense retrieval from the SQuAD 2.0 corpus, and (3) Llama fine-tuned on SQuAD 2.0 using QLoRA. Our experimental design isolates the effect of knowledge integration paradigm on abstention behavior while maintaining comparable model capacity and training data. Beyond standard metrics (EM, F1), we measure answer grounding using BERTScore \citep{zhang2020bertscore} and Semantic Textual Similarity (STS) \citep{cer2017semeval} to assess whether answers derive from provided/retrieved context versus parametric memory.

\textbf{Results preview}. Our evaluation reveals distinct tradeoffs between RAG and fine-tuning for unanswerable question detection...
