\begin{abstract}

Question answering systems must recognize when questions are unanswerable to avoid hallucinating responses, yet empirical guidance on knowledge integration paradigms for abstention remains limited. We present a comparison of Retrieval-Augemented Generation (RAG) versus parameter-efficient fine-tuning (QLoRA) for unanswerable question detection, evaluating how external versus internal knowledge integration affects abstention behavior in generative LLMs. Using SQuAD 2.0 and Llama-3.2-3B-Instruct, we find distinct accuracy-abstention tradeoffs: QLoRA achieves the highest unanswerable detection (74.53\% EM) but suffers degradation on answerable questions (2.99\% EM), while RAG provides a relatively more balanced performance (59.61\% unanswerable EM, 22.22\% answerable EM). Beyond standard metrics, we employ BERTScore and Semantic Textual Similarity to assess answer grounding. Our findings suggest that externalizing knowledge boundaries through retrieval yields more graceful degradation than parametric encoding, where models learn abstention as a dominant strategy, motivating future hybrid approaches.

\end{abstract}