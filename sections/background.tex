\section{Background}

\textbf{SQuAD datasets}. Question answering systems have evolved significantly with large-scale benchmarks. \citet{rajpurkar2016squad1} introduced SQuAD 1.0 with 100,000+ answerable questions from Wikipedia. However, SQuAD 1.0 had a critical limitation: every question was guaranteed to be answerable from the given context, meaning systems never needed to recognize when they lacked sufficient information. To address this, \citet{rajpurkar2018squad2} released SQuAD 2.0, adding 50,000 unanswerable questions. This established evaluating both answer accuracy and the ability to recognize knowledge boundaries - the capability we use to assess how different knowledge integration methods handle unanswerable questions.

\textbf{LLM hallucination}. The tendency of LLMs to hallucinate—generate plausible yet unfactual content—is a fundamental challenge in deployment \citep{ji2023hallucinate}. While post-generation detection methods exist \citep{farquhar2024semantic, sadat2023delucionqa}, our work examines prevention through different knowledge integration paradigms—specifically comparing how RAG versus fine-tuning affect hallucination rates when questions are unanswerable.

\textbf{Retrieval-Augmented Generation}. \citet{lewis2020rag} introduced RAG, which retrieves relevant documents using dense passage retrieval, then conditions generation on query and retrieved passages. This provides non-parametric knowledge access—information stored externally and provided at inference rather than encoded in weights. \citet{shuster2021raghallucination} found RAG reduces fabricated information in dialogue, suggesting external knowledge helps models distinguish accessible versus lacking information.

\textbf{Fine-tuning}. Fine-tuning represents an alternative paradigm where domain knowledge is encoded directly into model parameters through continued training on task-specific data. \citet{roberts2020fine} demonstrated fine-tuning effectively injects factual knowledge into parameters for closed-book QA. This parametric approach internalizes knowledge within the model weights themselves.

\textbf{PEFT}. Parameter-efficient methods like LoRA \citep{hu2022lora} and QLoRA \citep{dettmers2023qlora} made this practical for large models by fine-tuning only small matrices while freezing pretrained weights. These enable training on domain data including unanswerable examples, potentially teaching boundary recognition. We adopt QLoRA in our fine-tuning experiments to ensure computational feasibility while maintaining model quality.

\textbf{RAG-FT comparisons}. \citet{ovadia2024ftvsrag} compared paradigms across knowledge-intensive tasks, finding strengths depend on task structure and that RAG excels with less popular knowledge while fine-tuning benefits from abundant training data. However, their evaluation emphasized answerable question performance using exact match and F1 scores, not abstention on unanswerable questions.

\textbf{Our positioning}. We address a gap in existing comparisons: systematic evaluation of how RAG versus fine-tuning handle unanswerable question detection in generative LLMs. While prior comparisons \citep{ovadia2024ftvsrag} focus on accuracy when answers exist, we examine abstention behavior using SQuAD 2.0's unanswerable questions. Our controlled experimental setup compares three systems—zero-shot baseline, RAG with dense retrieval, and QLoRA fine-tuning—using the same base model and training data to isolate the effect of the knowledge integration paradigm. Beyond standard metrics (EM, F1), we use BERTScore \citep{zhang2020bertscore} and STS \citep{cer2017semeval} to measure whether generated answers are grounded in provided/retrieved context versus hallucinated from parametric memory, directly quantifying the distinction between external and internal knowledge access.