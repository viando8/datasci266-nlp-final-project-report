\section{Background}

Question answering systems have evolved significantly with large-scale benchmarks. \citet{rajpurkar2016squad1} introduced the Stanford Question Answering Dataset (SQuAD 1.0), a reading comprehension benchmark containing over 100,000 questions answerable from Wikipedia passages. However, SQuAD 1.0 had a critical limitation: every question was guaranteed to be answerable from the given context, meaning systems never needed to recognize when they lacked sufficient information. To address this, \citet{rajpurkar2018squad2} released SQuAD 2.0, adding 50,000 unanswerable questions. This established evaluating both answer accuracy and the ability to recognize knowledge boundaries - a capability we use to assess how different knowledge integration methods handle unanswerable questions.

The tendency of LLMs to hallucinate - generate false information when they lack knowledge - is a fundamental deployment challenge. \citet{kalai2025hallucinate} provides theoretical and empirical characterization of why LLMs hallucinate on unanswerable questions, demonstrating that models often prefer generating plausible-sounding responses over admitting ignorance. While researchers have developed post-generation detection methods \citep{farquhar2024semantic, sadat2023delucionqa}, these address hallucinations after they occur rather than preventing them through system design - a key distinction from our work, which compares prevention strategies through different knowledge integration paradigms.

Th consequences of hallucination vary dramatically by domain, with particularly severe implications in healthcare and legal applications. \citet{pal2023medhalt} demonstrates .