\subsection{Baseline Model}

Our baseline system answers questions from the SQuAD 2.0 validation dataset without retrieval or fine-tuning, using Llama-3.2-3B-Instruct \citep{grattafiori2024llama} in a zero-shot configuration. Each validation example is formatted with three components: (1) a task instruction defining expected behavior, (2) the context paragraph from SQuAD 2.0, and (3) the question.

\textbf{Instruction Design}. We evaluate three instruction variants on a stratified sample of 2,000 training examples to identify the optimal prompt formulation. Based on BERTScore F1 performance, we select the instruction variant designed to be more "explicit", emphasizing extractive behavior and providing clear abstention guidelines (see Appendix~\ref{app:baseline_prompt} for full methodology and selected instruction).

\textbf{Decoding Strategy}. The model must either extract an answer span from the context or output ``unanswerable'' when the context provides insufficient information. We employ low-temperature, top-p-constrained decoding (\texttt{temperature=0.1}, \texttt{top\_p=0.9}, \texttt{do\_sample=True}) to enforce extractive behavior, setting \texttt{max\_new\_tokens=35} based on the distribution of answer token counts in the training dataset.

\textbf{Output Parsing}. Generated outputs are parsed by extracting text following the ``Answer:`` delimiter; responses containing keywords such as ``unanswerable`` or ``cannot answer`` are classified as abstentions.
