\section{Introduction}

Question answering (QA) represents a primary use of large language models (LLMs), especially in domains that require accurate information. However, QA systems face a fundamental challenge: they often hallucinate answers to unanswerable questions instead of abstaining \citep{ji2023hallucinate}.

This challenge becomes critical in high-stakes domains. Healthcare systems for clinical decision support must never fabricate medical advice, as hallucinations can adversely affect patient health and care quality \citep{pal2023medhalt}. Legal research systems that invent non-existence cases have resulted in attorney sanctions \citep{dahl2024legal}. These domains particularly need robust unanswerable question detection because their knowledge bases have clear boundaries - medical guidelines cannot address every rare disease combination; legal databases cannot cover every novel situation.

Two paradigms address hallucination through different knowledge integration approaches. Retrieval-Augmented Generation (RAG) retrieves relevant passages before generation, providing external, non-parametric knowledge access \citep{lewis2020rag}. Fine-tuning encodes knowledge directly into model parameters through training on domain-specific data, representing internal, parametric integration \citep{roberts2020fine}.

Organizations deploying QA systems must choose between these paradigms yet lack empirical guidance on which better handles unanswerable questions. We address: \textit{Do models more reliably abstain when knowledge is provided externally (RAG) or encoded internally (fine-tuning)? How does each paradigm balance answering correctly while recognizing knowledge boundaries?}

\textbf{Gap in existing work}. While prior work compares RAG and fine-tuning for QA over low-frequency entities \citep{soudani2024ftvsrag}, their evaluation focuses on accuracy when answers exist in the knowledge base, not on abstention when questions are unanswerable. Hallucination detection work addresses fabrications after generation \citep{sadat2023delucionqa, farquhar2024semantic}, rather than comparing prevention through different knowledge integration approaches. SQuAD 2.0 studies concentrate on architectural improvements to extractive models \citep{rajpurkar2018squad2}, not paradigm comparisons for generative LLMs.

\textbf{Our contribution}. We present a systematic comparison of RAG versus fine-tuning focused on unanswerable question detection. Using SQuAD 2.0, we evaluate three systems: (1) zero-shot baseline with no retrieval or fine-tuning, (2) RAG system with dense retrieval from the SQuAD 2.0 corpus, and (3) Llama fine-tuned on SQuAD 2.0 using QLoRA. We examine how the different knowledge integration paradigms affect abstention behavior, controlling for model capacity and training data. Beyond standard metrics (EM, F1), we measure answer grounding using BERTScore \citep{zhang2020bertscore} and Semantic Textual Similarity (STS) \citep{cer2017semeval} to assess whether answers derive from provided context versus parametric memory.

\textbf{Results preview}. Our evaluation reveals critical behavioral differences between RAG and fine-tuning for unanswerable question detection. While both approaches improve abstention over zero-shot baseline of 52.01\%, QLoRA fine-tuning learned abstention as a dominant strategy: it achieved the highest unanswerable detection score (74.53\%) only by sacrificing nearly all utility on answerable questions (dropping to 2.99\%). In contrast, RAG provided a more robust balance (59.61\% unanswerable, 22.22\% answerable). These results indicate that fine-tuning leads to overfitting on abstention. These findings motivate future work on hybrid approaches that combine retrieval's external grounding with fine-tuning's abstention behavior. Code is available at \url{https://github.com/viando8/datasci266-nlp-final-project/tree/main}.