\section{Results and Discussion}

Table~\ref{tab:models_comparison} presents the performance comparison between the zero-shot baseline Llama-3.2-3B-Instruct model, the RAG-based system, and the QLoRA fine-tuned variant across all evaluation metrics.\footnote{Baseline and RAG metrics are computed on the full SQuAD 2.0 validation set (11,873 examples), while QLoRA metrics are from a 2,000-example subset. Future work will evaluate all models on identical subsets for stricter comparison.}

\begin{table*}[t]
\centering
\caption{Baseline vs. RAG vs. QLoRA fine-tuned model performance on SQuAD 2.0.}
\label{tab:models_comparison}
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Metric} & \multicolumn{2}{c}{\textbf{Baseline 3B}} & \multicolumn{2}{c}{\textbf{RAG 3B}} & \multicolumn{2}{c}{\textbf{QLoRA 3B}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Ans. & Unans. & Ans. & Unans. & Ans. & Unans. \\
\midrule
Exact Match & 38.36 & 52.01 & 22.22 & 59.61 & 2.99 & 74.53 \\
F1 Score & 59.58 & 52.01 & 39.41 & 59.61 & 18.93 & 74.53 \\
BERTScore F1 & 46.45 & 44.26 & 24.23 & 52.60 & 10.08 & 67.50 \\
STS Score & 85.83 & 78.41 & 76.50 & 81.87 & 71.14 & 88.38 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Comparing Baseline with RAG}

We evaluated both the Zero-Shot Baseline (Llama 3B) and the RAG System on the SQuAD 2.0 validation set. The analysis focuses on three key areas: overall performance, the ability to answer correctly when possible, and the ability to abstain when necessary.

The most significant finding is that the RAG system is significantly superior at handling unanswerable questions (+7.60\% improvement in EM). The RAG system acts "safely": when it cannot find a semantic match in the retrieved documents, it correctly abstains nearly 60\% of the time. 

However, the RAG system suffered a significant performance drop on Answerable questions (22.22\% EM vs. Baseline's 38.36\%). This highlights a critical "retrieval bottleneck." Even when the context window was expanded (from k=3 to k=5), the system frequently failed to generate the correct answer. This suggests that when the retriever fails to surface the exact evidence required, the LLM is forced to abstain due to strict prompt instructions, whereas the Baseline can leverage its internal training data to answer correctly.

Additionally, the Baseline achieved a higher BERTScore F1 (45.35\%) compared to RAG (38.57\%). This indicates that when the Baseline generates an answer, it is semantically closer to the ground truth. The lower score for RAG implies that when it does attempt to answer (rather than abstaining), the retrieved context might occasionally mislead the generation.

In summary, the RAG architecture successfully met the project's primary objective of improving unanswerable question detection. By explicitly grounding the generation in retrieved context, we reduced the rate of hallucination on unanswerable queries compared to the baseline. However, this came at the cost of sensitivity; the system became over-cautious, frequently abstaining on questions it could have answered, simply because the retrieval step was imperfect.

\subsection{Comparing Baseline with Q-Lora}

The QLoRA fine-tuned model reveals a critical failure mode in teaching LLMs to
recognize knowledge boundaries through low-resource fine-tuning. While
fine-tuning achieved the highest unanswerable detection score---Exact Match rose
from 52.01\textbackslash \% to 74.53\textbackslash \%, with BERTScore F1
increasing from 44.26\textbackslash \% to 67.50\textbackslash \%---this
improvement was not due to learned discrimination but rather because the model
learned abstention as a dominant strategy. This behavior is evidenced by the
catastrophic collapse in answerable question performance: Answerable Exact Match
fell from 38.36\textbackslash \% (Baseline) to just 2.99\textbackslash \%
(QLoRA). Similarly, the F1 score for answerable questions dropped to 18.
93\textbackslash \%. The Semantic Textual Similarity (STS) scores reinforce this
pattern: while the model achieved high similarity on unanswerable questions (88.
38\textbackslash \%), this reflects a tendency to output ``unanswerable''
regardless of the input. This asymmetry suggests that within our constrained
training regime (8,000 randomly sampled examples, single epoch), the model
resorted to the shortcut of always abstaining, rendering it functionally useless
for actual question answering. The finding highlights a key challenge for
fine-tuning approaches: optimizing for unanswerable question detection may
inadvertently suppress the extractive capabilities that make QA systems useful,
a tradeoff that retrieval-augmented approaches may handle differently by
externalizing knowledge boundaries.
